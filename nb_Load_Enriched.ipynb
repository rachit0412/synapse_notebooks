{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row, functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from delta.tables import *\n",
        "from datetime import *\n",
        "import time\n",
        "import pytz\n",
        "import pyodbc\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Configuration needs to be set to workaround issue of inserting dates before 1900\n",
        "spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load source file to dataframe\n",
        "def load_source_to_df(path, source, file_format=True):\n",
        "\n",
        "    if source == '<source_system>':\n",
        "        if file_format == True: # If file_format left empty, it will automatically try to retrieve the format from the files\n",
        "            files = mssparkutils.fs.ls(path)\n",
        "            if len(files) > 1:\n",
        "                file_format = files[1].name.split('.')[-1]\n",
        "            else:\n",
        "                file_format = files[0].name.split('.')[-1]                       \n",
        "    print(\"File format is \" + file_format)\n",
        "    df = spark.read.load(path=path,format=file_format)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_primarykeys(df_meta, table):\n",
        "\n",
        "    print(\"Extracting primary keys.....\")\n",
        "    # Filter DD03L on Table & Primary Keys\n",
        "    df_meta_filtered = df_meta \\\n",
        "                        .where((col(\"TABNAME\") == table) & (col(\"KEYFLAG\") == \"X\") & (col(\"DATATYPE\") != \" \")) \\\n",
        "                        .orderBy(col(\"POSITION\"))\n",
        "\n",
        "    # Put Primary Keys in list\n",
        "    primary_keys = df_meta_filtered \\\n",
        "                    .select(\"FIELDNAME\") \\\n",
        "                    .rdd.flatMap(lambda x: x) \\\n",
        "                    .collect()\n",
        "\n",
        "    # CODE IS NEEDED TO REMOVE KEYS WHICH ARE NOT PROVIDED BY SAP -> MANDT etc\n",
        "    print(\"Primary keys successfully extracted.\")\n",
        "    return primary_keys\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create SCD2 columns\n",
        "def add_scd2(df, ts, pks):\n",
        "\n",
        "    print(\"Implementing SCD2.....\")\n",
        "    columns_to_hash = df.columns\n",
        "    columns_to_hash.sort()\n",
        "\n",
        "    pks_to_hash = pks # Primary Keys are already sorted in metadata\n",
        "\n",
        "    df = df \\\n",
        "        .withColumn('hash', F.sha2(F.concat_ws(\"\", *columns_to_hash), 256)) \\\n",
        "        .withColumn('pk_hash', F.sha2(F.concat_ws(\"\", *pks_to_hash), 256))\n",
        "\n",
        "    # Set start_date to current datetime & end_date to default end date\n",
        "    start_date = ts.strftime('%Y-%m-%d.%H:%M:%S')\n",
        "    end_date = None\n",
        "\n",
        "    df = (df\n",
        "        .withColumn('Record_Start_Date', F.to_timestamp(F.lit(start_date), 'yyyy-MM-dd.HH:mm:ss'))\n",
        "        .withColumn('Record_End_Date', F.to_timestamp(F.lit(end_date), 'yyyy-MM-dd.HH:mm:ss'))\n",
        "        .withColumn('IsActive_Record', F.lit('Y'))\n",
        "    )\n",
        "    print(\"SCD2 successfully implemented.\")\n",
        "    return df\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to check whether the delta table exists\n",
        "def file_exists(path):\n",
        "  try:\n",
        "    mssparkutils.fs.ls(path)\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    if 'java.io.FileNotFoundException' in str(e):\n",
        "      return False\n",
        "    else:\n",
        "      raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to delete folders and files recursively\n",
        "def remove_folder(path):\n",
        "  if file_exists(path):\n",
        "    mssparkutils.fs.rm(path, True)\n",
        "  else:\n",
        "    raise Exception(\"Folder doesn't exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set dtypes\n",
        "def set_dtypes(df, df_metadata, table_name):\n",
        "\n",
        "    # Filter on table\n",
        "    df_metadata_table = df_metadata.filter(F.col('TABNAME')==table_name)\n",
        "\n",
        "    # Convert column dtypes in df based on df_metadata\n",
        "    df = df.select(*(F.col(c).cast(df_metadata_table.filter(F.col('FIELDNAME')==c).select('dtype').collect()[0][0])\n",
        "            .alias(c) if df_metadata_table.filter(F.col('FIELDNAME')==c).select('dtype').collect() else F.col(c).cast('string')\n",
        "            .alias(c) for c in df.columns))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to do a full load\n",
        "def write_full_load(df, destination_path, table_name, ts):\n",
        "\n",
        "    print(\"Extraction type is FULL.....\")\n",
        "\n",
        "    if file_exists(destination_path): # Upsert into existing delta table\n",
        "\n",
        "        # Set end date\n",
        "        end_date = ts +  timedelta(seconds=-1)\n",
        "\n",
        "        # Load existing delta table to df (Only the active records)\n",
        "        deltaTable = DeltaTable.forPath(spark, destination_path)\n",
        "        deltatable_df = deltaTable.toDF().where(\"IsActive_Record = 'Y'\")\n",
        "\n",
        "        # Find the records that are new or changed\n",
        "        df_toBeAdded = df.alias(\"toBeAdded\") \\\n",
        "            .join(deltatable_df.alias(\"deltaTable\"), on=\"hash\", how=\"left_outer\") \\\n",
        "            .where(\"deltaTable.hash IS NULL\") \\\n",
        "            .selectExpr(\"toBeAdded.*\")\n",
        "\n",
        "        # Find the records that have to be closed (due to deletion or update)\n",
        "        df_toBeDeleted = (df.alias(\"toBeDeleted\") \n",
        "            .join(deltatable_df.alias(\"deltaTable\"), on=\"hash\", how=\"right_outer\") \n",
        "            .where(\"toBeDeleted.hash IS NULL\")  # Meaning: these hashes exist in the current delta table (will be matched in the merge later on)\n",
        "            .selectExpr(\"deltaTable.*\"))\n",
        "\n",
        "        # Union the new, changed and to be closed records\n",
        "        df_stagedUpdates = (\n",
        "            df_toBeAdded\n",
        "                .selectExpr(\"toBeAdded.*\")\n",
        "                .union(df_toBeDeleted.selectExpr(\"*\"))\n",
        "        )\n",
        "\n",
        "        # Merge the stagedUpdates with the deltaTable.\n",
        "        # When records match on the hash the record is not in the new file anymore and has to be closed.\n",
        "        # current is set to false and valid_to is set to the current date.\n",
        "        # This closes this record.\n",
        "        # When there is no match, insert all columns of those record.\n",
        "        # This inserts changed records as new record with current = true and valid_from is null.\n",
        "        deltaTable.alias(\"deltaTable\") \\\n",
        "            .merge(df_stagedUpdates.alias(\"stagedUpdates\"), # Merge the toBeDeleted records based on hash\n",
        "                    \"deltaTable.hash = stagedUpdates.hash\") \\\n",
        "            .whenMatchedUpdate(\n",
        "            set={\n",
        "                \"IsActive_Record\": F.lit('N'),\n",
        "                \"Record_End_Date\": F.lit(end_date) #F.to_timestamp(F.lit(end_date), 'yyyy-MM-dd.HH:mm:ss')\n",
        "            }\n",
        "        ) \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()\n",
        "\n",
        "    else: # Create new table\n",
        "        df.write.mode(\"overwrite\").format(\"delta\").save(destination_path)\n",
        "\n",
        "    print(\"Extraction completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to do a delta load\n",
        "def write_delta_load(df, destination_path, table_name, ts):\n",
        "\n",
        "    print(\"Extraction type is DELTA.....\")\n",
        "    if file_exists(destination_path): # Upsert into existing delta table\n",
        "\n",
        "        # Set end date\n",
        "        end_date = ts +  timedelta(seconds=-1)\n",
        "\n",
        "        # Load existing delta table to df (Only the active records)\n",
        "        deltaTable = DeltaTable.forPath(spark, destination_path)\n",
        "        deltatable_df = deltaTable.toDF().where(\"IsActive_Record = 'Y'\")\n",
        "\n",
        "        # df (new delta load) should consist of only new and modified records \n",
        "        # Re-order columns so pk_hash is in front which will prevent issues when unioning & inserting the data\n",
        "        df_toBeAdded = df.alias(\"toBeAdded\") \\\n",
        "            .join(deltatable_df.alias(\"deltaTable\"), on=\"pk_hash\", how=\"left_outer\") \\\n",
        "            .where(\"(deltaTable.pk_hash IS NULL OR toBeAdded.hash <> deltaTable.hash)\") \\\n",
        "            .selectExpr(\"toBeAdded.*\")\n",
        "\n",
        "        #columns_to_front = [\"pk_hash\"]\n",
        "        #original = df_toBeAdded.columns\n",
        "        #original.remove(columns_to_front[0])\n",
        "        #df_toBeAdded = df_toBeAdded.select(*columns_to_front, *original)\n",
        "\n",
        "        # To be deleted needs to be matched based on pk_hash\n",
        "        df_toBeDeleted = df.alias(\"toBeDeleted\") \\\n",
        "            .join(deltatable_df.alias(\"deltaTable\"), on=\"pk_hash\", how=\"right_outer\") \\\n",
        "            .where(\"toBeDeleted.pk_hash IS NOT NULL AND (toBeDeleted.hash <> deltaTable.hash)\") \\\n",
        "            .selectExpr(\"deltaTable.*\")\n",
        "\n",
        "        # New set only contains new and modified records. Modified records have to be linked to existing records so that they can be closed. Modified records have PKs that already exist in the current delta table. Meaning, when comparing the\n",
        "        # two sets (delta and existing table), rows with overlapping records (= same PKs) are records that have to be closed in the existing table.\n",
        "\n",
        "        # Union the new, changed and to be closed records\n",
        "        df_stagedUpdates = (\n",
        "            df_toBeAdded\n",
        "                .selectExpr(\"toBeAdded.*\")\n",
        "                .union(df_toBeDeleted.selectExpr(\"*\"))\n",
        "        )\n",
        "\n",
        "        # Merge the stagedUpdates with the deltaTable.\n",
        "        # When records match on the hash the record is not in the new file anymore and has to be closed.\n",
        "        # current is set to false and valid_to is set to the current date.\n",
        "        # This closes this record.\n",
        "        # When there is no match, insert all columns of those record.\n",
        "        # This inserts changed records as new record with current = true and valid_from is null.\n",
        "        deltaTable.alias(\"deltaTable\") \\\n",
        "            .merge(df_stagedUpdates.alias(\"stagedUpdates\"), # Merge the toBeDeleted records based on pk_hash & hash -> also on hash because those are the records which did not change and need to be closed\n",
        "                    \"deltaTable.pk_hash = stagedUpdates.pk_hash AND deltaTable.hash = stagedUpdates.hash\") \\\n",
        "            .whenMatchedUpdate(\n",
        "            set={\n",
        "                \"IsActive_Record\": F.lit('N'),\n",
        "                \"Record_End_Date\": F.lit(end_date)\n",
        "            }\n",
        "        ) \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()\n",
        "\n",
        "    else: \n",
        "        # Create new table\n",
        "        df.write.mode(\"overwrite\").format(\"delta\").save(destination_path)\n",
        "    print(\"Extraction completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Environment to be passed as parameter\n",
        "env = 'td'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if env == 'td':\n",
        "    storageaccount = 'storageaccounttest'\n",
        "else:\n",
        "    storageaccount = 'storageaccountprod'\n",
        "\n",
        "# Source\n",
        "source = '<source_system>'\n",
        "source_container = 'raw'\n",
        "source_folder = f\"abfss://{source_container}@{storageaccount}.dfs.core.windows.net/{source}/data/\"\n",
        "\n",
        "# Destination\n",
        "destination_container = 'enriched'\n",
        "destination_folder = f\"abfss://{destination_container}@{storageaccount}.dfs.core.windows.net/{source}/data/\"\n",
        "\n",
        "# Archive\n",
        "archive_container = 'backup'\n",
        "archive_folder = f\"abfss://{archive_container}@{storageaccount}.dfs.core.windows.net/{source}/processed/\"\n",
        "\n",
        "# Metadata path - SAP Table DD03L\n",
        "meta_path = f\"abfss://{source_container}@{storageaccount}.dfs.core.windows.net/{source}/data/dd03l\"\n",
        "\n",
        "# Keyl vault\n",
        "key_vault_name = 'KV-'+env.upper()+'DATAHUB'\n",
        "\n",
        "# Metadata DB\n",
        "meta_server = 'tcp:sql-'+env+'-weu-GDWH-datahub.database.windows.net'\n",
        "meta_database = 'SQLDB-METADATASTOREDB'\n",
        "# meta_password_secret_name = 'sqlserveradmin'\n",
        "\n",
        "# Set timezone\n",
        "timezone = pytz.timezone('Europe/Amsterdam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Connect to metadata DB\n",
        "ls_key_vault_name = 'ls_AzureKeyVault'\n",
        "meta_username = 'sqlserveradmin'\n",
        "meta_password = mssparkutils.credentials.getSecret(key_vault_name,meta_username,ls_key_vault_name) \n",
        "driver = '{ODBC Driver 17 for SQL Server}'\n",
        "cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+meta_server+';PORT=1433;DATABASE='+meta_database+';UID='+meta_username+';PWD='+ meta_password)\n",
        "\n",
        "# Read metadata for tables using:\n",
        "# 1) SAP Table connector\n",
        "\n",
        "tables = pd.read_sql(\n",
        "    '''SELECT \n",
        "\t[TABLE_ID] AS [ID]\n",
        "\t,[TABLE_NAME]\n",
        "\t,[SOURCE_SYSTEM]\n",
        "\t,[EXTRACTION_TYPE]\n",
        "\t,[IS_ACTIVE]\n",
        "\t,'SAP TABLE' AS [CONNECTOR_TYPE]\n",
        "FROM [MDL].[EXTRACTION_SAPTABLE]\n",
        "WHERE [IS_ACTIVE] = '1'\n",
        "AND TABLE_NAME in ('EKET','EKKO','EKPO','EKES')\n",
        "\t'''\n",
        "# AND TABLE_ID IN (3,4,5,6) --Only for testing\n",
        "    ,cnxn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load active tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set start Timestamp\n",
        "ts_start = datetime.now(tz = timezone)\n",
        "\n",
        "# Load DD03L to df\n",
        "# Logging start\n",
        "print(f\"-------------------------\")\n",
        "print(f\"Metadata table: DDL03 from source system <source_system> is started at {ts_start}\")\n",
        "df_meta = load_source_to_df(meta_path, '<source_system>')\n",
        "# Set end Timestamp\n",
        "ts_end = datetime.now(tz = timezone)\n",
        "print(f\"Metadata table: DDL03 is finished at {ts_end}\")\n",
        "\n",
        "# Loop over tables and write them (full or delta load) as (1) delta tables with (2) SCD2 to enhanced folder\n",
        "for i, row in tables.iterrows():\n",
        "    table = row.TABLE_NAME\n",
        "    table_lower_case = row.TABLE_NAME.lower()\n",
        "    source_system = row.SOURCE_SYSTEM\n",
        "    extraction_type = row.EXTRACTION_TYPE\n",
        "\n",
        "    # Reset start Timestamp\n",
        "    ts_start = datetime.now(tz = timezone)\n",
        "\n",
        "    # Logging start\n",
        "    print(f\"-------------------------\")\n",
        "    print(f\"Source table: {table} from source system {source_system} is started at {ts_start}\")\n",
        "\n",
        "    source_path = f\"{source_folder}/{table_lower_case}/\"\n",
        "    archive_path = f\"{archive_folder}/{table_lower_case}/\"\n",
        "    destination_path = f\"{destination_folder}/{table_lower_case}/\"\n",
        "\n",
        "    print(\"Loading source table to dataframe.....\")\n",
        "    df = load_source_to_df(source_path, '<source_system>')\n",
        "    print(\"Loading successfully completed.\")\n",
        "\n",
        "    primary_keys = get_primarykeys(df_meta, table)\n",
        "\n",
        "    df = add_scd2(df, ts_start, primary_keys)\n",
        "\n",
        "    if extraction_type == \"FULL\":\n",
        "        write_full_load(df, destination_path, table, ts_start)\n",
        "\n",
        "    else:\n",
        "        write_delta_load(df, destination_path, table, ts_start) # extraction_type = DELTA or DELTATOKEN\n",
        "        \n",
        "\n",
        "    print(f\"Moving files from the source to backup.....\")\n",
        "    staging_files = mssparkutils.fs.ls(source_path)\n",
        "    for file in staging_files:\n",
        "        mssparkutils.fs.mkdirs(archive_path)\n",
        "        mssparkutils.fs.mv(file.path, archive_path, False) #Note: I've tried to use this command to create the directory but it did not work. hence the reason of adding the above command. \n",
        "    print(f\"Files successully moved.\")\n",
        "\n",
        "    # Reset end Timestamp\n",
        "    ts_end = datetime.now(tz = timezone)\n",
        "\n",
        "    # Logging end\n",
        "    print(f\"Source table: {table} is finished at {ts_end}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
